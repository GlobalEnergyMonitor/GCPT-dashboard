{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited by Taylor and working on Thursday, Sept 7th 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key parameters\n",
    "gcpt_dash_file = 'GCPT dashboard data 2023-08.xlsx'\n",
    "gcpt_dash_path = '/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/GOGPT-Dashboard/data/'\n",
    "\n",
    "# max_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import gspread\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/GCPT-dashboard/data_gcpt/\n"
     ]
    }
   ],
   "source": [
    "max_year = 2023\n",
    "\n",
    "client_secret = \"Desktop/GEM_INFO/client_secret.json\"\n",
    "client_secret_full_path = os.path.expanduser(\"~/\") + client_secret\n",
    "\n",
    "gcpt_dash_file = 'GCPT dashboard data 2023-08.xlsx'\n",
    "gcpt_dash_path = os.path.expanduser('/Users/gem-tah/Desktop/GEM_INFO/GEM_WORK/GCPT-dashboard/data/')\n",
    "print(gcpt_dash_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcpt_dash_xl = pd.ExcelFile(gcpt_dash_path + gcpt_dash_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gspread_access_file_read_only(key, title):\n",
    "    \"\"\"\n",
    "    key = Google Sheets unique key in the URL\n",
    "    title = name of the sheet you want to read\n",
    "    \"\"\"\n",
    "    gspread_creds = gspread.oauth(\n",
    "        scopes=[\"https://www.googleapis.com/auth/spreadsheets.readonly\"],\n",
    "        credentials_filename=client_secret_full_path,\n",
    "        # authorized_user_filename=json_token_name,\n",
    "    )\n",
    "    gsheets = gspread_creds.open_by_key(key)\n",
    "    # Access a specific tab\n",
    "    spreadsheet = gsheets.worksheet(title)\n",
    "    # expected_header option provided following: https://github.com/burnash/gspread/issues/1007\n",
    "    df = pd.DataFrame(spreadsheet.get_all_records(expected_headers=[]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_max_year(gcpt_dash_file):\n",
    "    yyyy_mm = gcpt_dash_file.split('.xlsx')[0].split(' ')[-1]\n",
    "    month_int = int(yyyy_mm.split('-')[-1])\n",
    "    year_int = int(yyyy_mm.split('-')[0])\n",
    "    if month_int < 6:\n",
    "        max_year = year_int - 1\n",
    "    else:\n",
    "        max_year = year_int\n",
    "        \n",
    "    # print(f\"max_year: {max_year}\")\n",
    "\n",
    "    return max_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of countries to choose from (GEM country names)\n",
    "# data in gcpt_status is most complete; example of Albania, which only has cancelled units, nothing else\n",
    "max_year = determine_max_year(gcpt_dash_file)\n",
    "gcpt_status_orig = pd.read_excel(gcpt_dash_xl, sheet_name = f'2014-{max_year}')\n",
    "gcpt_country_list = gcpt_status_orig['Country'].sort_values().unique().tolist()\n",
    "gcpt_country_list = ['all'] + gcpt_country_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map (upper left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_code_df():\n",
    "    # TO DO: change below to draw from GEM's country list, which also includes the ISO names\n",
    "    # \"GEM Naming Conventions - Countries & subdivisions\"\n",
    "    # https://docs.google.com/spreadsheets/d/1mtlwSJfWy1gbIwXVgpP3d6CcUEWo2OM0IvPD6yztGXI/edit#gid=0\n",
    "\n",
    "    # country_codes_file = 'List of ISO 3166 country codes (Wikipedia).xlsx'\n",
    "    # country_codes_path = '/Users/masoninman/Dropbox/GEM/'\n",
    "    # country_codes = pd.read_excel(country_codes_path + country_codes_file)\n",
    "\n",
    "\n",
    "    gem_naming_convention_key = '1mtlwSJfWy1gbIwXVgpP3d6CcUEWo2OM0IvPD6yztGXI'\n",
    "\n",
    "    # gspread way\n",
    "    country_codes = gspread_access_file_read_only(gem_naming_convention_key, 'Countries')\n",
    "\n",
    "    # clean up the codes to remove non-printing characters from wikipedia\n",
    "    for col in country_codes.columns:\n",
    "        if country_codes[col].dtype == object:\n",
    "            country_codes[col] = country_codes[col].str.replace('\\xa0', '', regex=False)\n",
    "\n",
    "    # get rid of parenthetical footnotes at end of names & whitespace\n",
    "    country_codes['ISO 3166 Country Name'] = country_codes['ISO 3166 Country Name'].str.split('[').str[0].str.strip()\n",
    "\n",
    "    return country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_name_conversions_dict():\n",
    "    \"\"\"\n",
    "    Creates a dictionary for converting GEM standard country names to ISO 3166 versions.\n",
    "    \n",
    "    This downloads the GEM standard names file from Google Sheets using pygsheets,\n",
    "    then pares down the df to only those in which GEM uses a different name than ISO 3166.\n",
    "\n",
    "    Then it creates a dictionary, which can be used for converting from GEM to ISO.\n",
    "    \n",
    "    This is needed because Plotly's choropleth function uses ISO names for getting the outline for each country.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    standard_country_names_key = '1mtlwSJfWy1gbIwXVgpP3d6CcUEWo2OM0IvPD6yztGXI'\n",
    "\n",
    "    # gspread way\n",
    "    df = gspread_access_file_read_only(standard_country_names_key, 'Countries')\n",
    "    # keep only those with a mismatch\n",
    "    name_diffs = df.copy()\n",
    "    name_diffs = name_diffs[name_diffs['GEM name same as ISO 3166?']=='FALSE']\n",
    "    # exclude those not in ISO\n",
    "    name_diffs = name_diffs[name_diffs['ISO 3166 Country Name']!='NOT LISTED']\n",
    "    name_diffs_dict = name_diffs.set_index('GEM Standard Country Name')['ISO 3166 Country Name'].to_dict()   \n",
    "    \n",
    "\n",
    "    \n",
    "    return name_diffs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These countries were missing: ['all']\n"
     ]
    }
   ],
   "source": [
    "# get map df\n",
    "gcpt_map = pd.read_excel(gcpt_dash_xl, sheet_name = 'Map')\n",
    "gcpt_map = gcpt_map.rename(columns={'Total operating Capacity': 'Capacity (MW)'})\n",
    "\n",
    "# add any missing countries, to make sure that all countries in gcpt_country_list are in gcpt_map\n",
    "missing_countries = [x for x in gcpt_country_list if x not in gcpt_map['Country'].tolist()]\n",
    "if len(missing_countries) > 0:\n",
    "    print(f\"These countries were missing: {missing_countries}\")\n",
    "    missing_df = pd.DataFrame.from_dict({\n",
    "            'Country': missing_countries, \n",
    "            'Capacity (MW)': [float(0)]*len(missing_countries)\n",
    "        }, orient='columns')\n",
    "    gcpt_map = pd.concat([gcpt_map, missing_df], sort=False)\n",
    "\n",
    "# change GEM country names to ISO 3166\n",
    "name_diffs_dict = create_country_name_conversions_dict()\n",
    "gcpt_map['ISO 3166 Country Name'] = gcpt_map['Country'].replace(name_diffs_dict)\n",
    "\n",
    "# Note: Kosovo isn't recognized in ISO 3166, so can't be shown on Plotly map on its own. \n",
    "# We could combine it with Serbia for display--but then our data in the dashboard wouldn't be the same as in our spreadsheets & other maps.\n",
    "# Wikipedia said Kosovo declared independence from Serbia in 2008; it is only partially recognized.\n",
    "\n",
    "# show countries in gcpt_map not in gcpt_country_list:\n",
    "extraneous_countries = [x for x in gcpt_map['Country'].tolist() if x not in gcpt_country_list]\n",
    "if len(extraneous_countries) > 0:\n",
    "    print(f\"Extraneous countries to be removed: {extraneous_countries}\")\n",
    "\n",
    "# keep only countries that are in gcpt_country_list\n",
    "gcpt_map = gcpt_map[gcpt_map['Country'].isin(gcpt_country_list)]\n",
    "\n",
    "# merge in ISO country codes (needed by Plotly)\n",
    "country_codes = create_country_code_df()\n",
    "gcpt_map = pd.merge(\n",
    "    country_codes[['ISO 3166 Country Name', 'Country ISO 3166-1 alpha-3']],\n",
    "    gcpt_map,\n",
    "    on='ISO 3166 Country Name', \n",
    "    how='outer'\n",
    ")\n",
    "gcpt_map = gcpt_map.rename(columns={'Country ISO 3166-1 alpha-3': 'iso_alpha'})\n",
    "\n",
    "# exclude those with no value for iso_alpha\n",
    "# This excludes notes within the ISO dataset, e.g., \"Akrotiri and Dhekelia â€“ See United Kingdom, The.\"\n",
    "# Unfortunately, this also excludes Kosovo from GCPT\n",
    "gcpt_map = gcpt_map[gcpt_map['iso_alpha'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if capacity is 0, instead use 1, to avoid zero capacity leading to -inf log value\n",
    "# similar approach is used in numpy log1p\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.log1p.html\n",
    "gcpt_map['capacity log10 + 1'] = np.log10(gcpt_map['Capacity (MW)'].replace(float(0), float(1)))\n",
    "\n",
    "# create hover text\n",
    "# (note: variable hover_text below is a Pandas Series)\n",
    "hover_text = gcpt_map['Country'] + ': '\n",
    "hover_text = hover_text + gcpt_map['Capacity (MW)'].map('{:,.0f}'.format) + ' MW'\n",
    "# hide extra bit, e.g. 'trace 0'; based on https://plotly.com/python/reference/#scatter-hovertemplate\n",
    "hover_text = hover_text + '<extra></extra>'\n",
    "gcpt_map['hover_text'] = hover_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_clean_data(df):\n",
    "    for col in df.columns:\n",
    "        if col not in ['Country', 'Capacity (MW)']:\n",
    "            # make the text lowercase\n",
    "            print(f\"Cleaning col {col}\")\n",
    "            df[col] = df[col].str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_status(df):\n",
    "    \"\"\"\n",
    "    convert column 'Status' to categorical\n",
    "    https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "    \"\"\"\n",
    "    \n",
    "    status_order = [\n",
    "        'operating',\n",
    "        'mothballed',\n",
    "        'announced',\n",
    "        'pre-permit',\n",
    "        'permitted',\n",
    "        'construction',\n",
    "        'retired',\n",
    "        'shelved',\n",
    "        'cancelled',\n",
    "    ]\n",
    "    df['Status'] = df['Status'].astype(\n",
    "        CategoricalDtype(status_order, ordered=False)\n",
    "    )    \n",
    "    df = df.sort_values(by=['Country', 'Status', 'Year'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_test_capacity(df, df_init):\n",
    "    \"\"\"Runs at end of status_condense, to check that there was no change in the data.\n",
    "    (Comparing version after condensing the data as it was before condensing the data.)\n",
    "    \n",
    "    There are entries with status NaN in the version of the data before condensing,\n",
    "    because the table prior to that step has a row for each unit, and columns for each year.\n",
    "    (Other distinguishing features for each unit aren't included in this table.)\n",
    "    \n",
    "    If there is no data for a unit for some of the years covered, then we have status NaN for those years.\n",
    "    (Example: It was proposed only in 2018, then the years 2014-2017 have status NaN.)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.reset_index()\n",
    "        \n",
    "    all_statuses = []\n",
    "    status_min_year = df['Year'].min()\n",
    "    for year in range(status_min_year, max_year+1):\n",
    "        all_statuses += df_init[year].tolist()\n",
    "    all_statuses = list(set(all_statuses))\n",
    "\n",
    "    for sel_status in all_statuses:\n",
    "        print(f\"Testing capacities for {sel_status}\")\n",
    "        for sel_year in range(status_min_year, max_year+1):\n",
    "            sel_init = df_init[df_init[sel_year]==sel_status]\n",
    "            sel_init_sum = sel_init['Capacity (MW)'].sum()\n",
    "\n",
    "            # handle two different formats for df\n",
    "            if 'Status' in df.columns:\n",
    "                sel = df[(df['Year']==sel_year) & (df['Status']==sel_status)]\n",
    "                sel_sum = sel['Capacity (MW)'].sum()\n",
    "                \n",
    "            else:\n",
    "                sel = df[df[sel_year]==sel_status]\n",
    "                sel_sum = sel['Capacity (MW)'].sum()\n",
    "\n",
    "            # compare values\n",
    "            abs_diff = abs(sel_sum - sel_init_sum)\n",
    "            if abs_diff <= 1e-7:\n",
    "                pass\n",
    "\n",
    "            elif abs_diff > 1e-7:\n",
    "                print(\"Error!\" + f\" Capacity difference for {sel_year} & {sel_status}.\")\n",
    "                print(f\"Initial value: {sel_init_sum}\")\n",
    "                print(f\"Current value: {sel_sum}\")\n",
    "            else:\n",
    "                print(\"Unexpected case\")\n",
    "    \n",
    "    # no return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_fill_in_missing(df):\n",
    "    \"\"\"\n",
    "    For each country, for each status, fill in zero value if it is missing.\n",
    "    \"\"\"\n",
    "    year_range = range(df['Year'].min(), df['Year'].max() + 1)\n",
    "    statuses = df['Status'].dropna().unique().tolist()\n",
    "    for country in df['Country'].unique().tolist():\n",
    "        for status in statuses:\n",
    "            for year in year_range:\n",
    "                df_sel = df[(df['Year']==year) & (df['Country']==country) & (df['Status']==status)]\n",
    "                if len(df_sel) == 0:\n",
    "                    # there is a missing value; fill it in\n",
    "                    # print(f\"To fill in missing value for {country}, {year}, {status}\") # for db\n",
    "                    # values in dict below must be lists, to avoid error:\n",
    "                    # \"ValueError: If using all scalar values, you must pass an index\"\n",
    "                    new_df = pd.DataFrame.from_dict({\n",
    "                                'Country': [country],\n",
    "                                'Year': [year],\n",
    "                                'Status': [status],\n",
    "                                'Capacity (MW)': [float(0)],\n",
    "                            }, orient='columns')\n",
    "                    df = pd.concat([df, new_df], sort=False)\n",
    "\n",
    "    df = df.sort_values(by=['Country', 'Year', 'Status'])\n",
    "                    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_calculate_global_totals(df):    \n",
    "    gcpt_status_all = df.groupby(['Year', 'Status'])[['Capacity (MW)']].sum().reset_index()\n",
    "    gcpt_status_all.insert(0, 'Country', 'all')\n",
    "    gcpt_status_all = sort_status(gcpt_status_all)\n",
    "    \n",
    "    df = pd.concat([gcpt_status_all, df], sort=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_condense(df_arg):\n",
    "    \"\"\"\n",
    "    There are sometimes duplicates, for example if two units in a plant have the exact same capacity and history.\n",
    "    So before setting the index to Country & Capacity, need to get rid of duplicates.\n",
    "    \"\"\"\n",
    "    df = df_arg.copy()\n",
    "    df_init = df_arg.copy()  # for test\n",
    "    \n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    df = df.set_index(['Country', 'Capacity (MW)'])\n",
    "    df = df.stack().reset_index()\n",
    "        \n",
    "    df = df.rename(columns={'level_2': 'Year', 0: 'Status'})\n",
    "    \n",
    "    df = df.groupby(['Country', 'Year', 'Status'])[['Capacity (MW)']].sum().reset_index()\n",
    "    df = sort_status(df)\n",
    "\n",
    "    # print(df.columns) # for db\n",
    "    status_test_capacity(df, df_init)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_statuses(gcpt_status_uncondensed):\n",
    "    accepted_statuses = ['announced', 'cancelled', 'construction', 'mothballed', 'operating', 'permitted',  'pre-permit', 'shelved', 'retired']\n",
    "    for col in range(2014, max_year +1):\n",
    "        ser = gcpt_status_uncondensed[col].dropna()\n",
    "        unaccepted = ser[~ser.isin(accepted_statuses)]\n",
    "        if len(unaccepted) > 0:\n",
    "            print(f\"Found unaccepted statuses; len(ser): {len(ser)}\")\n",
    "            print(unaccepted.value_counts())\n",
    "    # no return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning col 2014\n",
      "Cleaning col 2015\n",
      "Cleaning col 2016\n",
      "Cleaning col 2017\n",
      "Cleaning col 2018\n",
      "Cleaning col 2019\n",
      "Cleaning col 2020\n",
      "Cleaning col 2021\n",
      "Cleaning col 2022\n",
      "Cleaning col 2023\n",
      "Testing capacities for permitted\n",
      "Testing capacities for announced\n",
      "Testing capacities for pre-permit\n",
      "Testing capacities for nan\n",
      "Testing capacities for shelved\n",
      "Testing capacities for construction\n",
      "Testing capacities for cancelled\n",
      "Testing capacities for operating\n",
      "Testing capacities for mothballed\n",
      "Testing capacities for retired\n"
     ]
    }
   ],
   "source": [
    "gcpt_status_uncondensed = pd.read_excel(gcpt_dash_xl, sheet_name = f'2014-{max_year}')\n",
    "gcpt_status_uncondensed = status_clean_data(gcpt_status_uncondensed)\n",
    "test_statuses(gcpt_status_uncondensed)\n",
    "\n",
    "gcpt_status = status_condense(gcpt_status_uncondensed)\n",
    "gcpt_status = status_fill_in_missing(gcpt_status)\n",
    "gcpt_status = status_calculate_global_totals(gcpt_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age & Technology (lower left)\n",
    "- For bar chart of capacity by age & type\n",
    "- From the sheet 'Plant type and age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6564\n",
      "245\n"
     ]
    }
   ],
   "source": [
    "def age_read_and_clean(gcpt_dash_xl):\n",
    "    gcpt_age = pd.read_excel(gcpt_dash_xl, sheet_name = 'Plant type and age')\n",
    "    \n",
    "    # change capitalization of 'Ultra-Supercritical' to 'Ultra-supercritical'\n",
    "    gcpt_age['Combustion technology'] = gcpt_age['Combustion technology'].replace(\n",
    "        'Ultra-Supercritical', 'Ultra-supercritical'\n",
    "    )\n",
    "    \n",
    "    return gcpt_age\n",
    "\n",
    "def age_condense_data(df):\n",
    "    print(len(df))\n",
    "\n",
    "    # bin by decade\n",
    "    for row in df.index:\n",
    "        age = df.at[row, 'Plant Age']\n",
    "        if age < 10:\n",
    "            df.at[row, 'decade'] = '0-9 years'\n",
    "        elif age >= 10 and age < 20:\n",
    "            df.at[row, 'decade'] = '10-19 years'\n",
    "        elif age >= 20 and age < 30:\n",
    "            df.at[row, 'decade'] = '20-29 years'\n",
    "        elif age >= 30 and age < 40:\n",
    "            df.at[row, 'decade'] = '30-39 years'\n",
    "        elif age >= 40 and age < 50:\n",
    "            df.at[row, 'decade'] = '40-49 years'\n",
    "        elif age >= 50:\n",
    "            df.at[row, 'decade'] = '50+ years'\n",
    "        else:\n",
    "            print(\"Error!\" + f\" Issue with age for row {row}: {age}\")\n",
    "\n",
    "    df = df.groupby(['Country', 'decade', 'Combustion technology'])[['Capacity (MW)']].sum()\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # unstack, then fill in zeros\n",
    "    df = df.set_index(['Country', 'decade', 'Combustion technology']).unstack()\n",
    "    df = df.droplevel(0, axis=1)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df.columns.name = ''\n",
    "    \n",
    "    print(len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def age_calculate_global_totals(df):\n",
    "\n",
    "    technologies = ['CFB', 'IGCC', 'Subcritical', 'Supercritical', 'Ultra-supercritical', 'Unknown']\n",
    "    \n",
    "    gcpt_age_all = df.groupby('decade')[technologies].sum().reset_index()\n",
    "    gcpt_age_all.insert(0, 'Country', 'all')\n",
    "\n",
    "    df = pd.concat([gcpt_age_all, df], sort=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def age_fill_in_missing_decades(df):\n",
    "    decade_list = [\n",
    "        '0-9 years',\n",
    "        '10-19 years',\n",
    "        '20-29 years',\n",
    "        '30-39 years',\n",
    "        '40-49 years',\n",
    "        '50+ years',\n",
    "    ]\n",
    "    for country_sel in gcpt_country_list:\n",
    "        for decade_sel in decade_list:\n",
    "            df_sel = df[(df['Country']==country_sel) & (df['decade']==decade_sel)]\n",
    "            if len(df_sel) == 0:\n",
    "                new_df = pd.DataFrame.from_dict({\n",
    "                        'Country': country_sel,\n",
    "                        'decade': decade_sel,\n",
    "                        'CFB': [float(0)],\n",
    "                        'IGCC': [float(0)],\n",
    "                        'Subcritical': [float(0)],\n",
    "                        'Supercritical': [float(0)],\n",
    "                        'Ultra-supercritical': [float(0)],\n",
    "                        'Unknown': [float(0)],\n",
    "                    }, orient='columns')\n",
    "                df = pd.concat([df, new_df], sort=False)\n",
    "            elif len(df_sel) == 1:\n",
    "                pass\n",
    "\n",
    "            elif len(df_sel) > 1:\n",
    "                print(\"Error!\")\n",
    "            else:\n",
    "                print(\"Error! (of another kind)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "gcpt_age = age_read_and_clean(gcpt_dash_xl)\n",
    "gcpt_age = age_condense_data(gcpt_age)\n",
    "gcpt_age = age_calculate_global_totals(gcpt_age)\n",
    "gcpt_age = age_fill_in_missing_decades(gcpt_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additions & retirements\n",
    "- data for bar chart additions and retirements (lower-right)\n",
    "- sheet '2000-2022' (for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for bar chart additions and retirements (lower-right): sheet '2000-2022' (for example)\n",
    "# no condensing needed, but need to unstack\n",
    "\n",
    "additions_max_year = 2023\n",
    "\n",
    "gcpt_add = pd.read_excel(gcpt_dash_xl, sheet_name = f'2000-{additions_max_year}')\n",
    "\n",
    "\n",
    "def add_unstack(df):\n",
    "    df = df.set_index(['Country', 'Year', 'Status'])\n",
    "    df = df.unstack(-1)\n",
    "    df = df.droplevel(0, axis=1)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\n",
    "        'Operating': 'Added (MW)', \n",
    "        'Retired': 'Retired (MW)'\n",
    "    })\n",
    "        \n",
    "    return df\n",
    "\n",
    "def add_missing_countries(df):\n",
    "    # add any missing countries, to make sure that all countries in gcpt_country_list are in gcpt_map\n",
    "    # missing_countries = [x for x in gcpt_country_list if x not in gcpt_add['Country'].tolist()]\n",
    "    missing_countries = [x for x in gcpt_country_list if x not in gcpt_add['Country'].tolist()]\n",
    "\n",
    "    print(f\"Show any countries missing (which will be added below): {missing_countries}\")\n",
    "    \n",
    "    for year in range(\n",
    "        int(gcpt_add['Year'].min()), \n",
    "        int(gcpt_add['Year'].max())+1):\n",
    "\n",
    "        new_df = pd.DataFrame.from_dict({\n",
    "                'Country': missing_countries,\n",
    "                'Year': year,\n",
    "                'Added (MW)': [float(0)]*len(missing_countries),\n",
    "                'Retired (MW)': [float(0)]*len(missing_countries),\n",
    "            }, orient='columns')\n",
    "        df = pd.concat([df, new_df], sort=False)\n",
    "    return df\n",
    "\n",
    "def add_calculate_global_totals(df):\n",
    "    gcpt_add_all = df.groupby('Year')[['Added (MW)', 'Retired (MW)', 'Net added (MW)']].sum().reset_index()\n",
    "    gcpt_add_all.insert(0, 'Country', 'all')\n",
    "    df = pd.concat([gcpt_add_all, df], sort=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show any countries missing (which will be added below): ['all']\n"
     ]
    }
   ],
   "source": [
    "gcpt_add = add_unstack(gcpt_add)\n",
    "\n",
    "gcpt_add = add_missing_countries(gcpt_add)\n",
    "\n",
    "gcpt_add['Net added (MW)'] = gcpt_add['Added (MW)'].sub(gcpt_add['Retired (MW)'])\n",
    "gcpt_add = add_calculate_global_totals(gcpt_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEST: check that there are not any rows with NaNs for 'Net added'\n",
    "test = gcpt_add[gcpt_add['Net added (MW)'].isna()]\n",
    "if len(test) > 0:\n",
    "    print(\"Error!\" + f\" There were rows with value NaN for 'Net added (MW)'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to file: GCPT dashboard data 2023-08 - processed for Dash 2023-09-18_1649.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# export to various sheets of one Excel file by creating ExcelWriter object\n",
    "template_name = gcpt_dash_file.split('.xlsx')[0]\n",
    "save_timestamp = time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "file_name = f'{template_name} - processed for Dash {save_timestamp}.xlsx'\n",
    "with pd.ExcelWriter(gcpt_dash_path + file_name) as writer:\n",
    "    gcpt_map.to_excel(writer, sheet_name='map', index=False)  \n",
    "    gcpt_status.to_excel(writer, sheet_name='status', index=False)\n",
    "    gcpt_age.to_excel(writer, sheet_name='age', index=False)\n",
    "    gcpt_add.to_excel(writer, sheet_name='additions', index=False)\n",
    "    print(f\"Saved to file: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dashboardenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
